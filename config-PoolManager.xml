<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN"
                         "http://www.oasis-open.org/docbook/xml/4.3/docbookx.dtd" [
  <!ENTITY % sharedents SYSTEM "shared-entities.xml" >
  <!ENTITY psu    "<abbrev>PSU</abbrev>">
  <!ENTITY read   "<literal>read</literal>">
  <!ENTITY write  "<literal>write</literal>">
  <!ENTITY p2p    "<literal>p2p</literal>">
  <!ENTITY cache  "<literal>cache</literal>">

  %sharedents;
]>

<chapter id="cf-pm">

  <title>The &serv-poolmngr; Service</title>

  <!--
  <para>(We assume a basic knowledge of the functionality and terminology of
  dCache. Pool, domain, cell, how to log into a cell, and execute
  commands)</para>
  -->

  <para>
    The heart of a &dcache; system is the &serv-poolmngr;. When a user
    performs an action on a file - reading or writing - a
    <firstterm>transfer request</firstterm> is sent to the &dcache;
    system. The &serv-poolmngr; then decides how to handle this
    request.
  </para>

  <para>
    If a file the user wishes to read resides on one of the
    storage-pools within the &dcache; system, it will be transferred
    from that pool to the user. If it resides on several pools, the
    file will be retrieved from one of the pools determined by a
    configurable load balancing policy. If all pools the file is
    stored on are busy, a new copy of the file on an idle pool will be
    created and this pool will answer the request.
  </para>

  <para>
    A new copy can either be created by a <firstterm>pool to pool
    transfer</firstterm> (<abbrev>p2p</abbrev>) or by fetching it from
    a connected <firstterm>tertiary storage system</firstterm>
    (sometimes called <abbrev>HSM</abbrev> - hierarchical storage
    manager). Fetching a file from a tertiary storage system is called
    <firstterm>staging</firstterm>. It is also performed if the file
    is not present on any of the pools in the &dcache; system. The pool
    manager has to decide on which pool the new copy will be
    created, i.e. staged or p2p-copied.
  </para>

  <para>
    The behaviour of the &serv-poolmngr; service is highly
    configurable. In order to exploit the full potential of the
    software it is essential to understand the mechanisms used and how
    they are configured. The &serv-poolmngr; service creates the
    &cell-poolmngr; cell, which is a unique cell in &dcache; and
    consists of several sub-modules: The important ones are the
    <firstterm>pool selection unit</firstterm>
    (<abbrev>&psu;</abbrev>) and the load balancing policy as defined
    by the <firstterm>partition manager</firstterm>
    (<abbrev>PM</abbrev>).
  </para>

  <para>
    The &serv-poolmngr; can be configured by either directly editing
    the file <filename>&file-poolmanager;</filename>
    or via the <link linkend="intouch-admin">Admin
    Interface</link>. Changes made via the Admin Interface will be
    saved in the file
    <filename>&file-poolmanager;</filename> by the
    <command>save</command> command. This file will be parsed,
    whenever the &dcache; starts up. It is a simple text file
    containing the corresponding Admin Interface commands. It can
    therefore also be edited before the system is started. It can also
    be loaded into a running system with the <command>reload</command>
    command.  In this chapter we will describe the commands allowed in
    this file.
<!-- TODO:
This needs to be explained in the section on the Admin Interface:
        The syntax of the commands for configuring the &psu; will be
	explained with the examples below. These commands can be
	issued within the &cell-poolmngr; to change the configuration
	while the system is running. The
	<command>save</command>-command can then be used to save the
	current configuration to the file
	<filename>&file-poolmanager;</filename>.
-->
  </para>


  <section id="cf-pm-psu">
    <title>The Pool Selection Mechanism</title>

    <para>
      The &psu; is responsible for finding the set of pools which can
      be used for a specific transfer-request. By telling the &psu;
      which pools are permitted for which type of transfer-request,
      the administrator of the &dcache; system can adjust the system
      to any kind of scenario: Separate organizations served by
      separate pools, special pools for writing the data to a tertiary
      storage system, pools in a DMZ which serves only a certain kind
      of data (e.g., for the grid). This section explains the
      mechanism employed by the &psu; and shows how to configure it
      with several examples.
    </para>

    <para>
      The &psu; generates a list of allowed storage-pools for each
      incoming transfer-request. The &psu; configuration described below
      tells the &psu; which combinations of transfer-request and
      storage-pool are allowed. Imagine a two-dimensional table with
      a row for each possible transfer-request and a column for each
      pool - each field in the table containing either
      <quote>yes</quote> or <quote>no</quote>. For an incoming
      transfer-request the &psu; will return a list of all pools with
      <quote>yes</quote> in the corresponding row.
    </para>

    <para>
      Instead of <quote>yes</quote> and <quote>no</quote> the table
      really contains a <firstterm>preference</firstterm> - a
      non-negative integer.  However, the &psu; configuration is easier
      to understand if this is ignored.
    </para>

    <para>
      Actually maintaining such a table in memory (and as user in a
      configuration file) would be quite inefficient, because there are
      many possibilities for the transfer-requests. Instead, the &psu;
      consults a set of rules in order to generate the list of allowed
      pools. Each such rule is called a <firstterm>link</firstterm>
      because it links a set of transfer-requests to a group of pools.
    </para>

    <section id='cf-pm-links'>
      <title>Links</title>
      <para>
	A link consists of a
	set of unit groups and a list of pools. If all the unit groups
	are matched, the pools belonging to the link are added to
	the list of allowable pools.
      </para>

      <para>
	A link is defined in the file
	<filename>&file-poolmanager;</filename> by
      </para>

	<cmdsynopsis>
	  <command>psu create link</command>
	  <arg choice='plain'><replaceable>link</replaceable></arg>
	  <arg choice='plain'><replaceable>unitgroup</replaceable></arg>

	  <command>psu set link</command>
	  <arg choice='plain'><replaceable>link</replaceable></arg>
	  <arg choice='plain'><parameter>-readpref</parameter>=<replaceable>rpref</replaceable></arg>
	  <arg choice='plain'><parameter>-writepref</parameter>=<replaceable>wpref</replaceable></arg>
	  <arg choice='plain'><parameter>-cachepref</parameter>=<replaceable>cpref</replaceable></arg>
	  <arg choice='plain'><parameter>-p2ppref</parameter>=<replaceable>ppref</replaceable></arg>

	  <command>psu add link</command>
	  <arg choice='plain'><replaceable>link</replaceable></arg>
	  <arg choice='plain'><replaceable>poolgroup</replaceable></arg>
	</cmdsynopsis>

      <para>
	For the preference values see <xref linkend="cf-pm-psu-pref"/>.
      </para>

      <para>
	The main task is to understand how the unit groups in a link
	are defined. After we have dealt with that, the preference
	values will be discussed and a few examples will follow.
      </para>

      <para>
	The four properties of a transfer request, which are relevant for
	the &psu;, are the following:
<!--      </para>
-->
	<variablelist>
          <varlistentry>
            <term>Location of the File</term>
            <listitem>
	      <para>
		The location of the file in the file system is not
		used directly. Each file has the following two
		properties which can be set per directory:
	      </para>

		<itemizedlist>
		  <listitem>
		    <formalpara>
		    <title>Storage Class</title>

			The storage class is a string. It is used by a
			tertiary storage system to decide where to
			store the file (i.e. on which set of tapes)
			and &dcache; can use the storage class for a
			similar purpose (i.e. on which pools the file
			can be stored.). A detailed description of the
			syntax and how to set the storage class of a
			directory in the namespace is given in <xref
			linkend="secStorageClass" />.
		    </formalpara>
		    </listitem>

		    <listitem>
		    <formalpara>
		      <title>Cache Class</title>
			The cache class is a string with essentially
			the same functionality as the storage class,
			except that it is not used by a tertiary
			storage system. It is used in cases, where the
			storage class does not provide enough
			flexibility. It should only be used, if an
			existing configuration using storage classes
			does not provide sufficient flexibility.
		      </formalpara>

		    </listitem>
		  </itemizedlist>

	    </listitem>
	  </varlistentry>

          <varlistentry>
            <term>IP Address</term>
            <listitem>
	      <para>
		The IP address of the requesting host.
	      </para>

            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Protocol / Type of Door</term>
            <listitem>
	      <para>
		The protocol respectively the type of door used by the transfer.
	      </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Type of Transfer</term>
            <listitem>
              <para>
		The type of transfer is either &read;, &write;, &p2p;
		request or &cache;.
	      </para>
	      <para>
		A request for reading a file which is not on a read
		pool will trigger a &p2p; request and a subsequent
		&read; request. These will be treated as two separate
		requests.
	      </para>
	      <para>
		A request for reading a file which is not stored on
		disk, but has to be staged from a connected tertiary
		storage system will trigger a &cache; request to fetch
		the file from the tertiary storage system and a
		subsequent &read; request. These will be treated as
		two separate requests.
	      </para>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>

     <para>
       Each link contains one or more <firstterm>unit
       groups</firstterm>, all of which have to be matched by the
       transfer request. Each unit group in turn contains several
       <firstterm>units</firstterm>. The unit group is matched if at
       least one of the units is matched.
      </para>

      <section id='cf-pm-links-units'>
	<title>Types of Units</title>
	<para>
	  There are four types of units: network
	  (<literal>-net</literal>), protocol
	  (<literal>-protocol</literal>), storage class
	  (<literal>-store</literal>) and cache class
	  (<literal>-dcache</literal>) units. Each type imposes a
	  condition on the IP address, the protocol, the storage class
	  and the cache class respectively.
	</para>
	<para>
	  For each transfer at most one of each of the four unit types
	  will match.  If more than one unit of the same type could
	  match the request then the most restrictive unit matches.
	</para>
	<para>
	  The unit that matches is selected from all units defined in
	  &dcache;, not just those for a particular unit group.  This
	  means that, if a unit group has a unit that could match a
	  request but this request also matches a more restrictive
	  unit defined elsewhere then the less restrictive unit will
	  not match.
	</para>

	<variablelist>

          <varlistentry>
            <term>Network Unit</term>
            <listitem>
	      <para>
		A <emphasis>network unit</emphasis> consists of an IP
		address and a net mask written as
		<literal><replaceable>IP-address</replaceable>/<replaceable>net
		mask</replaceable></literal>, say
		<literal>111.111.111.0/255.255.255.0</literal>. It
		is satisfied, if the request is coming from a host
		with IP address within the subnet given by the
		address/netmask pair.
	      </para>

	      <screen><command>psu create ugroup</command> <replaceable>name-of-unitgroup</replaceable>
<command>psu create unit</command><parameter> -net</parameter> <replaceable>IP-address</replaceable>/<replaceable>net mask</replaceable>
<command>psu addto ugroup</command> <replaceable>name-of-unitgroup</replaceable> <replaceable>IP-address</replaceable>/<replaceable>net mask</replaceable></screen>
	    </listitem>
          </varlistentry>

	  <varlistentry>
	    <term>Protocol Unit</term>
	    <listitem>
	      <para>
		A <emphasis>protocol unit</emphasis> consists of the
		name of the protocol and the version number written as
		<replaceable>protocol-name</replaceable>/<replaceable>version-number</replaceable>,
		e.g., <literal>xrootd/3</literal>.
	      </para>
		<screen><command>psu create ugroup</command> <replaceable>name-of-unitgroup</replaceable>
<command>psu create unit</command><parameter> -protocol</parameter> <replaceable>protocol-name</replaceable>/<replaceable>version-number</replaceable>
<command>psu addto ugroup</command> <replaceable>name-of-unitgroup</replaceable> <replaceable>protocol-name</replaceable>/<replaceable>version-number</replaceable></screen>

	    </listitem>
	  </varlistentry>

	  <varlistentry>
	    <term>Storage Class Unit</term>
	    <listitem>
	      <para>
		A <emphasis>storage class unit</emphasis> is given by
		a storage class. It is satisfied if the requested file
		has this storage class. Simple wild cards are allowed:
		for this it is important to know that a storage class
		must always contain exactly one
		<literal>@</literal>-symbol as will be explained in
		<xref linkend="secStorageClass" />. In a storage class
		unit, either the part before the
		<literal>@</literal>-symbol or both parts may be
		replaced by a <literal>*</literal>-symbol; for
		example, <literal>*@osm</literal> and
		<literal>*@*</literal> are both valid storage class
		units whereas <literal>something@*</literal> is
		invalid. The <literal>*</literal>-symbol represents a
		limited wildcard: any string that doesn't contain an
		<literal>@</literal>-symbol will match.
	      </para>
	      <screen><command>psu create ugroup</command> <replaceable>name-of-unitgroup</replaceable>
<command>psu create unit</command> <parameter>-store</parameter> <replaceable>StoreName</replaceable>:<replaceable>StorageGroup</replaceable>@<replaceable>type-of-storage-system</replaceable>
<command>psu addto ugroup</command> <replaceable>name-of-unitgroup</replaceable> <replaceable>StoreName</replaceable>:<replaceable>StorageGroup</replaceable>@<replaceable>type-of-storage-system</replaceable></screen>
	    </listitem>
	  </varlistentry>

	  <varlistentry>
	    <term>Cache Class Unit</term>
	    <listitem>
	      <para>
		A <emphasis>cache class unit</emphasis> is given by a
		cache class. It is satisfied, if the cache class of
		the requested file agrees with it.
	      </para>
	      <screen><command>psu create ugroup</command> <replaceable>name-of-unitgroup</replaceable>
<command>psu create unit</command> <parameter>-dcache</parameter> <replaceable>name-of-cache-class</replaceable>
<command>psu addto ugroup</command> <replaceable>name-of-unitgroup</replaceable> <replaceable>name-of-cache-class</replaceable></screen>
	    </listitem>
	  </varlistentry>

	</variablelist>

      </section>

      <section id="cf-pm-psu-pref">
	<title>Preference Values for Type of Transfer</title>
      <para>
	The conditions for the <emphasis>type of transfer</emphasis>
	are not specified with units. Instead, each link contains four
	attributes <literal>-readpref</literal>,
	<literal>-writepref</literal>,
	<literal>-p2ppref</literal> and
	<literal>-cachepref</literal>, which specify a
	preference value for the respective types of transfer. If all
	the unit groups in the link are matched, the corresponding
	preference is assigned to each pool the link points to. Since
	we are ignoring different preference values at the moment, a
	preference of <literal>0</literal> stands for
	<literal>no</literal> and a non-zero preference stands for
	<literal>yes</literal>. A negative value for <literal>-p2ppref</literal> means, that the value for <literal>-p2ppref</literal> should equal the one for the <literal>-readpref</literal>.
      </para>

      <section>
	<title>Multiple non-zero Preference Values</title>

	<note>
	  <para>
	    This explanation of the preference values can be skipped
	    at first reading. It will not be relevant, if all non-zero
	    preference values are the same. If you want to try
	    configuring the pool manager right now without bothering
	    about the preferences, you should only use
	    <literal>0</literal> (for <literal>no</literal>) and, say,
	    <literal>10</literal> (for <literal>yes</literal>) as
	    preferences. You can choose <literal>-p2ppref=-1</literal> if it should match the value for <literal>-readpref</literal>. The first examples below are of this type.
	  </para>
	</note>

	<para>
	  If several different non-zero preference values are used,
	  the &psu; will not generate a single list but a set of
	  lists, each containing pools with the same preference. The
	  Pool Manager will use the list of pools with highest
	  preference and select a pool according to the load balancing
	  policy for the transfer. Only if all pools with the highest
	  preference are offline, the next list will be considered by
	  the Pool Manager. This can be used to configure a set of
	  fall-back pools which are used if none of the other pools
	  are available.
	</para>
      </section>
    </section>

      <section>
        <title>Pool Groups</title>

        <para>
	  Pools can be grouped together to pool groups.
	</para>
	<screen><command>psu create pgroup</command> <replaceable>name-of-poolgroup</replaceable>
<command>psu create pool</command> <replaceable>name-of-pool</replaceable>
<command>psu addto pgroup</command> <replaceable>name-of-poolgroup</replaceable> <replaceable>name-of-pool</replaceable></screen>

	<informalexample>
	  <para>
	    Consider a host <literal>pool1</literal> with two pools,
	    <literal>pool1_1</literal> and <literal>pool1_2</literal>,
	    and a host <literal>pool2</literal> with one pool
	    <literal>pool2_1</literal>. If you want to treat them in
	    the same way, you would create a pool group and put all of
	    them in it:
	  </para>
	  <programlisting><command>psu create pgroup</command> normal-pools
<command>psu create pool</command> pool1_1
<command>psu addto pgroup</command> normal-pools pool1_1
<command>psu create pool</command> pool1_2
<command>psu addto pgroup</command> normal-pools pool1_2
<command>psu create pool</command> pool2_1
<command>psu addto pgroup</command> normal-pools pool2_1</programlisting>

          <para>
	    If you later want to treat <literal>pool1_2</literal>
	    differently from the others, you would remove it from this
	    pool group and add it to a new one:
	  </para>
	  <programlisting><command>psu removefrom pgroup</command> normal-pools pool1_2
<command>psu create pgroup</command> special-pools
<command>psu addto pgroup</command> special-pools pool1_2</programlisting>

	</informalexample>
	<para>
	  In the following, we will assume that the necessary pool
	  groups already exist. All names ending with
	  <literal>-pools</literal> will denote pool
	  groups.
	</para>

        <para>
	  Note that a pool-node will register itself with the
	  &cell-poolmngr;: The pool will be created within the &psu; and
	  added to the pool group
	  <literal>default</literal>, if that
	  exists. This is why the &dcache; system will automatically use
	  any new pool-nodes in the standard configuration: All pools
	  are in <literal>default</literal> and can
	  therefore handle any request.
	</para>
      </section>

    <section id="secStorageClass">
      <title>Storage Classes</title>

      <para>
	The storage class is a string of the form
	<literal><replaceable>StoreName</replaceable>:<replaceable>StorageGroup</replaceable>@<replaceable>type-of-storage-system</replaceable></literal>,
	where
	<literal><replaceable>type-of-storage-system</replaceable></literal>
	denotes the type of storage system in use, and
	<literal><replaceable>StoreName</replaceable></literal>:<literal><replaceable>StorageGroup</replaceable></literal>
	is a string describing the storage class in a syntax which
	depends on the storage system. In general use
	<literal><replaceable>type-of-storage-system</replaceable>=osm</literal>.
      </para>

      <para>
	Consider for example the following setup:
      </para>

      <informalexample>
	<screen>&prompt-root; <userinput>&chimera-cli; lstag /data/experiment-a</userinput>
Total: 2
OSMTemplate
sGroup
&prompt-root; <userinput>&chimera-cli; readtag /data/experiment-a OSMTemplate</userinput>
StoreName myStore
&prompt-root; <userinput>&chimera-cli; readtag /data/experiment-a sGroup</userinput>
STRING</screen>

       <para>
	 This is the setup after a fresh installation and it will lead
	 to the storage class
	 <literal>myStore:STRING@osm</literal>. An adjustment to more
	 sensible values will look like
       </para>
<screen>&prompt-root; <userinput>&chimera-cli; writetag /data/experiment-a OSMTemplate "StoreName exp-a"</userinput>
&prompt-root; <userinput>&chimera-cli; writetag /data/experiment-a sGroup "run2010"</userinput></screen>

      <para>
	and will result in the storage class
	<literal>exp-a:run2010@osm</literal> for any data stored in
	the <filename class='directory'>/data/experiment-a</filename>
	directory.
      </para>

      </informalexample>

      <para>
	To summarize: The storage class depends on the directory
	the data is stored in and is configurable.
      </para>

    </section>

    <section id="secCacheClass">
      <title>Cache Class</title>

      <para>
	Storage classes might already be in use for the configuration
	of a tertiary storage system. In most cases they should be
	flexible enough to configure the &psu;. However, in rare cases
	the existing configuration and convention for storage classes
	might not be flexible enough.
      </para>

      <para>
	Consider for example a situation, where data produced by an
	experiment always has the same storage class
	<literal>exp-a:alldata@osm</literal>. This is good for the
	tertiary storage system, since all data is supposed to go to
	the same tape set sequentially. However, the data also
	contains a relatively small amount of meta-data, which is
	accessed much more often by analysis jobs than the rest of the
	data. You would like to keep the meta-data on a dedicated set
	of &dcache; pools. However, the storage class does not provide
	means to accomplish that.
      </para>

      <para>
	The cache class of a directory is set by the tag
	<literal>cacheClass</literal> as follows:
      </para>

      <informalexample>
      <screen>&prompt-root; <userinput>&chimera-cli; writetag /data/experiment-a cacheClass "metaData"</userinput></screen>
      <para>
	In this example the meta-data is stored in directories which
	are tagged in this way.
      </para>
      </informalexample>

	<para>
	  Check the existing tags of a directory and their content by:
	</para>
	<screen>&prompt-root; <userinput>&chimera-cli; lstag /path/to/directory</userinput>
Total: numberOfTags
tag1
tag2
...
&prompt-root; <userinput>&chimera-cli; readtag /path/to/directory tag1</userinput>
contentOfTag1</screen>

	<note>
	  <para>
	    A new directory will inherit the tags from the parent
	    directory. But updating a tag will
	    <emphasis>not</emphasis> update the tags of any child
	    directories.
	  </para>
	</note>

    </section>

      <section>
	<title>Define a link</title>
	<para>
	  Now we have everything we need to define a link.
	</para>
	<screen><command>psu create ugroup</command> <replaceable>name-of-unitgroup</replaceable>
<command>psu create unit</command><parameter> - <replaceable>type</replaceable></parameter> <replaceable>unit</replaceable>
<command>psu addto ugroup</command> <replaceable>name-of-unitgroup</replaceable> <replaceable>unit</replaceable>

<command>psu create pgroup</command> <replaceable>poolgroup</replaceable>
<command>psu create pool</command> <replaceable>pool</replaceable>
<command>psu addto pgroup</command> <replaceable>poolgroup</replaceable> <replaceable>pool</replaceable>

<command>psu create link</command> <replaceable>link</replaceable> <replaceable>name-of-unitgroup</replaceable>
<command>psu set link</command> <replaceable>link</replaceable> <parameter>-readpref=</parameter><replaceable>10</replaceable> <parameter>-writepref=</parameter><replaceable>0</replaceable> <parameter>-cachepref=</parameter><replaceable>10</replaceable><parameter>-p2ppref=</parameter><replaceable>-1</replaceable>
<command>psu add link</command> <replaceable>link</replaceable>  <replaceable>poolgroup</replaceable>
</screen>
      </section>

    </section>

    <section>
      <title>Examples</title>
      <para>
	Find some examples for the configuration of the &psu; below.
      </para>

      <section id="secExReadWrite">
	<title>Separate Write and Read Pools</title>

	<para>
	  The &dcache; we are going to configure receives data from a
	  running experiment, stores the data onto a tertiary storage
	  system, and serves as a read cache for users who want to
	  analyze the data.  While the new data from the experiment
	  should be stored on highly reliable and therefore expensive
	  systems, the cache functionality may be provided by
	  inexpensive hardware. It is therefore desirable to have a
	  set of pools dedicated for writing the new data and a
	  separate set for reading.
	</para>

	<informalexample>
	  <para>
	    The simplest configuration for such a setup would consist
	    of two links <quote>write-link</quote> and
	    <quote>read-link</quote>. The configuration is as follows:
	</para>
	<programlisting><command>psu create pgroup</command> read-pools
<command>psu create pool</command> pool1
<command>psu addto pgroup</command> read-pools pool1
<command>psu create pgroup</command> write-pools
<command>psu create pool</command> pool2
<command>psu addto pgroup</command> write-pools pool2

<command>psu create unit</command><parameter> -net</parameter> 0.0.0.0/0.0.0.0
<command>psu create ugroup</command> allnet-cond
<command>psu addto ugroup</command> allnet-cond 0.0.0.0/0.0.0.0

<command>psu create link</command> read-link allnet-cond
<command>psu set link</command> read-link <parameter>-readpref=</parameter>10 <parameter>-writepref=</parameter>0 <parameter>-cachepref=</parameter>10
<command>psu add link</command> read-link read-pools

<command>psu create link</command> write-link allnet-cond
<command>psu set link</command> write-link <parameter>-readpref=</parameter>0 <parameter>-writepref=</parameter>10 <parameter>-cachepref=</parameter>0
<command>psu add link</command> write-link write-pools</programlisting>
        <para>
	  Why is the unit group <literal>allnet-cond</literal>
	  necessary? It is used as a condition which is always true in
	  both links. This is needed, because each link must contain
	  at least one unit group.
	</para>
	</informalexample>
      </section>

      <section>
        <title>Restricted Access by IP Address</title>

        <para>
	  You might not want to give access to the pools for the whole
	  network, as in the previous example (<xref
	  linkend="secExReadWrite" />), though.
	</para>
	<informalexample>
	  <para>
	    Assume, the experiment data is copied into the cache from
	    the hosts with IP <literal>111.111.111.201</literal>,
	    <literal>111.111.111.202</literal>, and
	    <literal>111.111.111.203</literal>. As you might guess,
	    the subnet of the site is
	    <literal>111.111.111.0/255.255.255.0</literal>. Access
	    from outside should be denied. Then you would modify the
	    above configuration as follows:
	</para>
<programlisting><command>psu create pgroup</command> read-pools
<command>psu create pool</command> pool1
<command>psu addto pgroup</command> read-pools pool1
<command>psu create pgroup</command> write-pools
<command>psu create pool</command> pool2
<command>psu addto pgroup</command> write-pools pool2

<command>psu create unit</command> <parameter>-store</parameter> *@*

<command>psu create unit</command> <parameter>-net</parameter> 111.111.111.0/255.255.255.0
<command>psu create unit</command> <parameter>-net</parameter> 111.111.111.201/255.255.255.255
<command>psu create unit</command> <parameter>-net</parameter> 111.111.111.202/255.255.255.255
<command>psu create unit</command> <parameter>-net</parameter> 111.111.111.203/255.255.255.255

<command>psu create ugroup</command> write-cond
<command>psu addto ugroup</command> write-cond 111.111.111.201/255.255.255.255
<command>psu addto ugroup</command> write-cond 111.111.111.202/255.255.255.255
<command>psu addto ugroup</command> write-cond 111.111.111.203/255.255.255.255

<command>psu create ugroup</command> read-cond
<command>psu addto ugroup</command> read-cond 111.111.111.0/255.255.255.0
<command>psu addto ugroup</command> read-cond 111.111.111.201/255.255.255.255
<command>psu addto ugroup</command> read-cond 111.111.111.202/255.255.255.255
<command>psu addto ugroup</command> read-cond 111.111.111.203/255.255.255.255

<command>psu create link</command> read-link read-cond
<command>psu set link</command> read-link <parameter>-readpref=</parameter>10 <parameter>-writepref=</parameter>0 <parameter>-cachepref=</parameter>10
<command>psu add link</command> read-link read-pools

<command>psu create link</command> write-link write-cond
<command>psu set link</command> write-link <parameter>-readpref=</parameter>0 <parameter>-writepref=</parameter>10 <parameter>-cachepref=</parameter>0
<command>psu add link</command> write-link write-pools</programlisting>

        <important>
	  <para>
	    For a given transfer exactly zero or one storage class
	    unit, cache class unit, net unit and protocol unit will
	    match. As always the most restrictive one will match, the
	    IP <literal>111.111.111.201</literal> will match the
	    <literal>111.111.111.201/255.255.255.255</literal> unit
	    and not the <literal>111.111.111.0/255.255.255.0</literal>
	    unit. Therefore if you only add
	    <literal>111.111.111.0/255.255.255.0</literal> to the unit
	    group <quote>read-cond</quote>, the transfer request
	    coming from the IP <literal>111.111.111.201</literal> will
	    only be allowed to write and not to read. The same is true
	    for transfer requests from <literal>111.111.111.202</literal> and
	    <literal>111.111.111.203</literal>.
	  </para>
	</important>
	</informalexample>
      </section>

      <section>
        <title>Reserving Pools for Storage and Cache Classes</title>

        <para>
	  If pools are financed by one experimental group, they
	  probably do not like it if they are also used by another
	  group. The best way to restrict data belonging to one
	  experiment to a set of pools is with the help of storage
	  class conditions. If more flexibility is needed, cache class
	  conditions can be used for the same purpose.
	</para>

	<informalexample>
	  <para>
	    Assume, data of experiment A obtained in 2010 is written
	    into subdirectories in the namespace tree which are tagged
	    with the storage class
	    <literal>exp-a:run2010@osm</literal>, and
	    similarly for the other years. (How this is done is
	    described in <xref linkend="secStorageClass" />.)
	    Experiment B uses the storage class
	    <literal>exp-b:alldata@osm</literal> for
	    all its data.  Especially important data is tagged with
	    the cache class
	    <literal>important</literal>. (This is
	    described in <xref linkend="secCacheClass" />.) A suitable
	    setup would be
	  </para>
	  <programlisting><command>psu create pgroup</command> exp-a-pools
<command>psu create pool</command> pool1
<command>psu addto pgroup</command> exp-a-pools pool1

<command>psu create pgroup</command> exp-b-pools
<command>psu create pool</command> pool2
<command>psu addto pgroup</command> exp-b-pools pool2

<command>psu create pgroup</command> exp-b-imp-pools
<command>psu create pool</command> pool3
<command>psu addto pgroup</command> exp-b-imp-pools pool3

<command>psu create unit</command> <parameter>-net</parameter> 111.111.111.0/255.255.255.0
<command>psu create ugroup</command> allnet-cond
<command>psu addto ugroup</command> allnet-cond 111.111.111.0/255.255.255.0

<command>psu create ugroup</command> exp-a-cond
<command>psu create unit</command> -store exp-a:run2011@osm
<command>psu addto ugroup</command> exp-a-cond exp-a:run2011@osm
<command>psu create unit</command> -store exp-a:run2010@osm
<command>psu addto ugroup</command> exp-a-cond exp-a:run2010@osm

<command>psu create link</command> exp-a-link allnet-cond exp-a-cond
<command>psu set link</command> exp-a-link <parameter>-readpref=</parameter>10 <parameter>-writepref=</parameter>10 <parameter>-cachepref=</parameter>10
<command>psu add link</command> exp-a-link exp-a-pools

<command>psu create ugroup</command> exp-b-cond
<command>psu create unit</command> <parameter>-store</parameter> exp-b:alldata@osm
<command>psu addto ugroup</command> exp-b-cond exp-b:alldata@osm

<command>psu create ugroup</command> imp-cond
<command>psu create unit</command> <parameter>-dcache</parameter> important
<command>psu addto ugroup</command> imp-cond important

<command>psu create link</command> exp-b-link allnet-cond exp-b-cond
<command>psu set link</command> exp-b-link <parameter>-readpref=</parameter>10 <parameter>-writepref=</parameter>10 <parameter>-cachepref=</parameter>10
<command>psu add link</command> exp-b-link exp-b-pools

<command>psu create link</command> exp-b-imp-link allnet-cond exp-b-cond imp-cond
<command>psu set link</command> exp-b-imp-link <parameter>-readpref=</parameter>20 <parameter>-writepref=</parameter>20 <parameter>-cachepref=</parameter>20
<command>psu add link</command> exp-b-link exp-b-imp-pools</programlisting>

          <para>
	    Data tagged with cache class
	    <quote><literal>important</literal></quote> will always be
	    written and read from pools in the pool group
	    <literal>exp-b-imp-pools</literal>, except when all pools
	    in this group cannot be reached. Then the pools in
	    <literal>exp-a-pools</literal> will be used.
	  </para>

	  <para>
	    Note again that these will never be used otherwise. Not
	    even, if all pools in <literal>exp-b-imp-pools</literal>
	    are very busy and some pools in
	    <literal>exp-a-pools</literal> have nothing to do and lots
	    of free space.
	  </para>
        </informalexample>

        <para>
	  The central IT department might also want to set up a few
	  pools, which are used as fall-back, if none of the pools of
	  the experiments are functioning. These will also be used for
	  internal testing. The following would have to be added to
	  the previous setup:
	</para>

	<informalexample>

	  <programlisting><command>psu create pgroup</command> it-pools
<command>psu create pool</command> pool_it
<command>psu addto pgroup</command> it-pools pool_it

<command>psu create link</command> fallback-link allnet-cond
<command>psu set link</command> fallback-link <parameter>-readpref=</parameter>5 <parameter>-writepref=</parameter>5 <parameter>-cachepref=</parameter>5
<command>psu add link</command> fallback-link it-pools</programlisting>

          <para>
	    Note again that these will only be used, if none of the
	    experiments pools can be reached, or if the storage class
	    is not of the form <literal>exp-a:run2009@osm</literal>,
	    <literal>exp-a:run2010@osm</literal>, or
	    <literal>exp-b:alldata@osm</literal>. If the administrator
	    fails to create the unit
	    <literal>exp-a:run2005@osm</literal> and add it to the
	    unit group <literal>exp-a-cond</literal>, the fall-back
	    pools will be used eventually.
	  </para>

	</informalexample>
      </section>
    </section>
 <!--   </section>
-->






  </section>

  <section id="cf-pm-pm">
    <title>The Partition Manager</title>

    <para>The partition manager defines one or more load balancing
    policies. Whereas the &psu; produces a prioritized set of
    candidate pools using a collection of rules defined by the
    administrator, the load balancing policy determines the specific
    pool to use. It is also the load balancing policy that determines
    when to fall back to lesser prirority links, or when to trigger
    creation of additional copies of a file.</para>

    <para>Since the load balancing policy and parameters are defined
    per partition, understanding the partition manager is essential to
    tuning load balancing. This does not imply that one has to
    partition the &dcache; instance. It is perfectly valid to use a
    single partition for the complete instance.</para>

    <para>This section documents the use of the partition manager, how
    to create partitions, set parameters and how to associate links
    with partitions. In the following sections the available partition
    types and their configuration parameters are described.</para>

    <section id="cf-pm-pm-overview">
        <title>Overview</title>

        <para>There are various parameters that affect the load
        balancing policy. Some of them are generic and apply to any
        load balancing policy, but many are specific to a particular
        policy. To avoid limiting the complete &dcache; instance to a
        single configuration, the choice of load balancing policy and
        the various parameters apply to
        <firstterm>partitions</firstterm> of the instance. The load
        balancing algorithm and the available parameters is determined
        by the <firstterm>partition type</firstterm>.</para>

        <para>Each &psu; link can be associated with a different
        partion and the policy and parameters of that partition will
        be used to choose a pool from the set of candidate pools. The
        only partition that exists without being explicitly created is
        the partition called <literal>default</literal>. This
        partition is used by all links that do not explicitly identify
        a partition. Other partitions can be created or modified as
        needed.</para>

        <para>The <literal>default</literal> partition has a
        hard-coded partition type called
        <literal>classic</literal>. This type implements the one load
        balancing policy that was available in &dcache; before version
        2.0. The <literal>classic</literal> partition type is
        described later. Other partitions have one of a number of
        available types.  The system is pluggable, meaning that third
        party plugins can be loaded at runtime and add additional
        partition types, thus providing the ultimate control over load
        balancing in &dcache;. Documentation on how to develop plugins
        is beyond the scope of this chapter.</para>

        <para>To ease the management of partition parameters, a common
        set of shared parameters can be defined outside all
        partitions. Any parameter not explicitly set on a partition
        inherits the value from the common set. If not defined in the
        common set, a default value determined by the partition type
        is used. Currently, the common set of parameters happens to be
        the same as the parameters of the <literal>default</literal>
        partition, however this is only due to compatibility
        constraints and may change in future versions.</para>
    </section>


    <section id="cf-pm-pm-commands">
        <title>Managing Partitions</title>

      <para>
	For each partition you can choose the load balancing
	policy. You do this by chosing the type of the partition.
      </para>

      <para>
	Currently four different partition types are supported:
      </para>

      <variablelist>
	<varlistentry>
	  <term><literal>classic</literal>:</term>
	  <listitem>
	    <para>
	      This is the pool selection algorithm used in the versions
	    of &dcache; prior to version <literal>2.0</literal>. See
	    <xref linkend='cf-pm-classic'/> for a detailed
	    description.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term><literal>random</literal>:</term>
	  <listitem>
	    <para>
	      This pool selection algorithm selects a pool randomly
	      from the set of available pools.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term><literal>lru</literal>:</term>
	  <listitem>
	    <para>
	      This pool selection algorithm selects the pool that has
	      not been used the longest.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term><literal>wass</literal>:</term>
	  <listitem>
	    <para>
	      This pool selection algorithm selects pools randomly
	      weighted by available space, while incorporating age and
	      amount of garbage collectible files and information
	      about load.
	    </para>
	    <para>
	      This is the partition type of the default partition.
	      See <ulink
	      url="http://www.dcache.org/articles/wass.html">How to
	      Pick a Pool</ulink> for more details.
	    </para>
	  </listitem>
	</varlistentry>
      </variablelist>

        <para>Commands related to &dcache; partitioning:</para>

        <itemizedlist>
            <listitem>
                <para>
                    <cmdsynopsis>
                        <command>pm types</command>
                    </cmdsynopsis>
                    Lists available partition types. New partition types
                    can be added through plugins.
                </para>
            </listitem>

            <listitem>
                <para>
                    <cmdsynopsis>
                        <command>pm create</command>
                        <arg choice='opt'>-type=<replaceable>partitionType</replaceable></arg>
                        <arg choice='plain'><replaceable>partitionName</replaceable></arg>
                    </cmdsynopsis>
                    Creates a new partition. If no partition type is
                    specified, then a <literal>wass</literal> partition
                    is created.
                </para>
            </listitem>

            <listitem>
                <para>
                    <cmdsynopsis>
                        <command>pm set</command>
                        <arg choice='opt'><replaceable>partitionName</replaceable></arg>
                        <arg choice='plain'>-<replaceable>parameterName</replaceable>
                        =<replaceable>value</replaceable>|off</arg>
                    </cmdsynopsis>
                    Sets a parameter
                    <replaceable>parameterName</replaceable> to a new
                    value.
                </para>

                <para>If <replaceable>partitionName</replaceable> is
                omitted, the common shared set of parameters is
                updated. The value is used by any partition for which
                the parameter is not explicitly set.</para>

                <para>If a parameter is set to <literal>off</literal>
                then this parameter is no longer defined and is
                inherited from the common shared set of parameters, or
                a partition type specific default value is used if the
                parameter is not defined in the common set.
                </para>
            </listitem>

            <listitem>
                <para>
                    <cmdsynopsis>
                        <command>pm ls</command>
                        <arg choice='opt'>-l</arg>
                        <arg choice='opt'><replaceable>partitionName</replaceable></arg>
                    </cmdsynopsis>
                    Lists a single or all partitions, including the
                    type of each partition. If a partition name or the
                    <literal>-l</literal> option are used, then the
                    partition parameters are shown too. Inherited and
                    default values are identified as such.
                </para>
            </listitem>

            <listitem>
                <para>
                    <cmdsynopsis>
                        <command>pm destroy</command>
                        <arg choice='plain'><replaceable>partitionName</replaceable></arg>
                    </cmdsynopsis>
                    Removes a partition from &dcache;. Any links
                    configured to use this partition will fall back to
                    the <literal>default</literal> partition.
                </para>
            </listitem>
        </itemizedlist>
    </section>
    <section id="cf-pm-pm-usingPartitions">
      <title>Using Partitions</title>

        <para>A partition, so far, is just a set of parameters which
        may or may not differ from the default set. To let a partition
        relate to a part of the &dcache;, links are used.  Each link
        may be assigned to exactly one partition. If not set, or the
        assigned partition doesn't exist, the link defaults to the
        <literal>default</literal> partition.</para>

        <cmdsynopsis>
            <command>psu set link</command>
            <arg><replaceable>linkName</replaceable></arg>
            <arg choice='plain'>-section=<replaceable>partitionName</replaceable></arg>
            <arg rep='repeat' choice='opt'><replaceable>other-options</replaceable></arg>
        </cmdsynopsis>

        <para>Whenever this link is chosen for pool selection, the
        associated parameters of the assigned partition will become
        active for further processing.</para>

        <warning>
            <para>Depending on the way links are setup it may very
            well happen that more than just one link is triggered for
            a particular &dcache; request. This is not illegal but
            leads to an ambiguity in selecting an appropriate &dcache;
            partition. If only one of the selected links has a
            partition assigned, this partition is chosen.  Otherwise,
            if different links point to different partitions, the
            result is indeterminate.  This issue is not yet solved and
            we recommend to clean up the &serv-poolmngr; configuration
            to eliminate links with the same preferences for the same
            type of requests.
            </para>
        </warning>

        <para>In the <link linkend="intouch-web">Web Interface</link> you
        can find a web page listing partitions and more information.  You
        will find a page summarizing the partition status of the
        system. This is essentially the output of the command <command>pm
        ls -l</command>.</para>

        <informalexample>
            <para>
                For your &dcache; on <systemitem
                class='domainname'>dcache.example.org</systemitem> the address is
            </para>
            <para>
                <uri>http://dcache.example.org:2288/poolInfo/parameterHandler/set/matrix/*</uri>
            </para>
        </informalexample>


    <section id="cf-pm-pm-examples">
        <title>Examples</title>
        <para>
            For the subsequent examples we assume a basic &serv-poolmngr;
            setup :
        </para>

        <informalexample>
<programlisting>#
# define the units
#
psu create unit -protocol   */*
psu create unit -protocol   xrootd/*
psu create unit -net        0.0.0.0/0.0.0.0
psu create unit -net        131.169.0.0/255.255.0.0
psu create unit -store      *@*
#
#  define unit groups
#
psu create ugroup  any-protocol
psu create ugroup  any-store
psu create ugroup  world-net
psu create ugroup  xrootd
#
psu addto ugroup any-protocol */*
psu addto ugroup any-store    *@*
psu addto ugroup world-net    0.0.0.0/0.0.0.0
psu addto ugroup desy-net     131.169.0.0/255.255.0.0
psu addto ugroup xrootd       xrootd/*
#
#  define the pools
#
psu create pool pool1
psu create pool pool2
psu create pool pool3
psu create pool pool4

#
#  define the pool groups
#
psu create pgroup default-pools
psu create pgroup special-pools
#
psu addto pgroup default-pools pool1
psu addto pgroup default-pools pool2
#
psu addto pgroup special-pools pool3
psu addto pgroup special-pools pool4
#</programlisting>
        </informalexample>

        <section>
            <title>Disallowing pool to pool transfers for special pool
            groups based on the access protocol</title>

            <para>
                For a special set of pools, where we only allow the &xrootd;
                protocol, we don't want the datasets to be replicated on high
                load while for the rest of the pools we allow replication on
                hot spot detection.
            </para>
            <informalexample>
<programlisting>#
pm create xrootd-section
#
pm set default        -p2p=0.4
pm set xrootd-section -p2p=0.0
#
psu create link default-link any-protocol any-store world-net
psu add    link default-link default-pools
psu set    link default-link -readpref=10 -cachepref=10 -writepref=0
#
psu create link xrootd-link xrootd any-store world-net
psu add    link xrootd-link special-pools
psu set    link xrootd-link -readpref=10 -cachepref=10 -writepref=0
psu set    link xrootd-link -section=xrootd-section
#        </programlisting>
            </informalexample>
        </section>

        <section>
            <title>Choosing pools randomly for incoming traffic only</title>

            <para>
                For a set of pools we select pools following the default
                setting of cpu and space related cost factors. For incoming
                traffic from outside, though, we select the same pools, but in
                a randomly distributed fashion.  Please
                note that this is not really a physical partitioning of the
                &dcache; system, but rather a virtual one, applied to the same
                set of pools.
            </para>
            <informalexample>
<programlisting>#
pm create incoming-section
#
pm set default          -cpucostfactor=0.2 -spacecostfactor=1.0
pm set incoming-section -cpucostfactor=0.0 -spacecostfactor=0.0
#
psu create link default-link any-protocol any-store desy-net
psu add    link default-link default-pools
psu set    link default-link -readpref=10 -cachepref=10 -writepref=10
#
psu create link default-link any-protocol any-store world-net
psu add    link default-link default-pools
psu set    link default-link -readpref=10 -cachepref=10 -writepref=0
#
psu create link incoming-link any-protocol any-store world-net
psu add    link incoming-link default-pools
psu set    link incoming-link -readpref=10 -cachepref=10 -writepref=10
psu set    link incoming-link -section=incoming-section
#</programlisting>
            </informalexample>
        </section>
    </section>

    </section>



  <section id="cf-pm-classic">
      <title>Classic Partitions</title>

      <para>The <literal>classic</literal> partition type implements
      the load balancing policy known from &dcache; releases before
      version 2.0. This partition type is still the default. This
      section describes this load balancing policy and the available
      configuration parameters.</para>

      <informalexample>
          <para>To create a classic partition use the command:
          <command>pm create</command> -type=classic
          <replaceable>partitionName</replaceable>
          </para>
      </informalexample>

      <section id="cf-pm-classic-policy">
          <title>Load Balancing Policy</title>

          <para>From the allowable pools as determined by the
          <glossterm linkend="gl-pm-comp-psu">pool selection
          unit</glossterm>, the pool manager determines the pool used
          for storing or reading a file by calculating a <glossterm
          linkend="gl-cost">cost</glossterm> value for each pool. The
          pool with the lowest cost is used.</para>

          <para>If a client requests to read a file which is stored on
          more than one allowable pool, the <glossterm
          linkend="gl-performance_cost">performance costs</glossterm>
          are calculated for these pools. In short, this cost value
          describes how much the pool is currently occupied with
          transfers.</para>

          <para>If a pool has to be selected for storing a file, which
          is either written by a client or <glossterm
          linkend="gl-restore">restored</glossterm> from a <glossterm
          linkend="gl-tape_backend">tape backend</glossterm>, this
          performance cost is combined with a <glossterm
          linkend="gl-space_cost">space cost</glossterm> value to a
          <glossterm linkend="gl-cost">total cost</glossterm> value
          for the decision. The space cost describes how much it
          <quote>hurts</quote> to free space on the pool for the file.
          </para>

          <para>The <glossterm linkend="gl-pm-comp-cm">cost
          module</glossterm> is responsible for calculating the cost
          values for all pools.  The pools regularly send all
          necessary information about space usage and request queue
          lengths to the cost module.  It can be regarded as a cache
          for all this information.  This way it is not necessary to
          send <quote>get cost</quote> requests to the pools for each
          client request. The cost module interpolates the expected
          costs until a new precise information package is coming from
          the pools.  This mechanism prevents clumping of requests.
          </para>

          <para>Calculating the cost for a data transfer is done in
          two steps. First, the cost module merges all information
          about space and transfer queues of the pools to calculate
          the performance and space costs separately.  Second, in the
          case of a write or stage request, these two numbers are
          merged to build the total cost for each pool. The first step
          is isolated within a separate loadable class. The second
          step is done by the partition.
          </para>
      </section>

      <section id="cf-pm-classic-perf">
          <title>The Performance Cost</title>

          <para>
              The load of a pool is determined by comparing the
              current number of active and waiting
              <glossterm>transfers</glossterm> to the maximum number
              of concurrent transfers allowed.

              This is done separately for each of the transfer types
              (<glossterm linkend="gl-store">store</glossterm>,
              <glossterm linkend="gl-restore">restore</glossterm>,
              pool-to-pool client, pool-to-pool server, and client
              request) with the following equation:
          </para>

          <para>
              perfCost(per Type) = ( activeTransfers + waitingTransfers ) / maxAllowed .
          </para>

          <para>
              The maximum number of concurrent transfers
              (<symbol>maxAllowed</symbol>) can be configured with the commands
              <xref linkend="cmd-st_set_max_active"/> (store),
              <xref linkend="cmd-rh_set_max_active"/> (restore),
              <xref linkend="cmd-mover_set_max_active"/> (client request),
              <xref linkend="cmd-p2p_set_max_active"/> (pool-to-pool server), and
              <xref linkend="cmd-pp_set_max_active"/> (pool-to-pool client).
              <!-- (NOCLUE: <command moreinfo="refentry"><xref linkend="cmd-flush_set_max_active"/></command>) -->

              <!-- NOCLUE: Some more info about the meanings of these parameters would be nice. -->
          </para>


          <para>
              Then the average is taken for each mover type where
              <symbol>maxAllowed</symbol> is not zero. For a pool
              where store, restore and client transfers are allowed,
              e.g.,
          </para>

          <para>
              perfCost(total) = ( perfCost(store) + perfCost(restore) + perfCost(client) ) / 3 ,
          </para>

          <para>
              and for a read only pool:
          </para>

          <para>
              perfCost(total) = ( perfCost(restore) + perfCost(client) ) / 2 .
          </para>

          <para>
              For a well balanced system, the performance cost should not exceed 1.0.
          </para>

      </section>

      <section id="cf-pm-classic-space">
          <title>The Space Cost</title>

          <para>
              In this section only the new scheme for calculating the space
              cost will be described. Be aware, that the old scheme will be
              used if the <glossterm linkend="gl-breakeven">breakeven
              parameter</glossterm> of a pool is larger or equal 1.0.
          </para>

          <para>
              The cost value used for determining a pool for storing a file
              depends either on the free space on the pool or on the age of
              the <glossterm linkend="gl-lru">least recently used (LRU)
              file</glossterm>, which whould have to be deleted.
          </para>

          <para>
              The space cost is calculated as follows:

              <informaltable frame="none">
                  <tgroup cols="6" colsep="none" rowsep="none" align="left">
                      <colspec colnum="1" colname="if" colwidth="20"/>
                      <colspec colnum="2" colname="formula" colwidth="5*"/>
                      <colspec colnum="3" colname="and" colwidth="25"/>
                      <colspec colnum="4" colname="formula" colwidth="3*"/>
                      <colspec colnum="5" colname="then" colwidth="25"/>
                      <colspec colnum="6" colname="formula" colwidth="8*"/>
                      <tbody>
                          <row>
                              <entry>If </entry>
                              <entry>
                                  <phrase role="math">
                                      freeSpace &gt; gapPara
                                  </phrase>
                              </entry>
                              <entry></entry>
                              <entry></entry>
                              <entry> then </entry>
                              <entry>
                                  <phrase role="math">
                                      spaceCost = 3 * newFileSize / freeSpace
                                  </phrase>
                              </entry>
                          </row>
                          <row>
                              <entry>If </entry>
                              <entry>
                                  <phrase role="math">
                                      freeSpace &lt;= gapPara
                                  </phrase>
                              </entry>
                              <entry>
                                  and
                              </entry>
                              <entry>
                                  <phrase role="math">
                                      lruAge &lt; 60
                                  </phrase>
                              </entry>
                              <entry> then </entry>
                              <entry>
                                  <phrase role="math">
                                      spaceCost = 1 + costForMinute
                                  </phrase>
                              </entry>
                          </row>
                          <row>
                              <entry>If </entry>
                              <entry>
                                  <phrase role="math">
                                      freeSpace &lt;= gapPara
                                  </phrase>
                              </entry>
                              <entry>
                                  and
                              </entry>
                              <entry>
                                  <phrase role="math">
                                      lruAge &gt;= 60
                                  </phrase>
                              </entry>
                              <entry> then </entry>
                              <entry>
                                  <phrase role="math">
                                      spaceCost = 1 + costForMinute * 60 / lruAge
                                  </phrase>
                              </entry>
                          </row>
                      </tbody>
                  </tgroup>
              </informaltable>

              where the variable names have the following meanings:

              <variablelist>
                  <varlistentry>
                      <term><phrase role="math">freeSpace</phrase></term>
                      <listitem>
                          <para>
                              The free space left on the pool
                          </para>
                      </listitem>
                  </varlistentry>
                  <varlistentry>
                      <term><phrase role="math">newFileSize</phrase></term>
                      <listitem>
                          <para>
                              The size of the file to be written to one of the pools, and at least 50MB.
                          </para>
                      </listitem>
                  </varlistentry>
                  <varlistentry>
                      <term><phrase role="math">lruAge</phrase></term>
                      <listitem>
                          <para>
                              The age of the <glossterm linkend="gl-lru">least
                              recently used file</glossterm> on the pool.
                          </para>
                      </listitem>
                  </varlistentry>
                  <varlistentry>
                      <term><phrase role="math">gapPara</phrase></term>
                      <listitem>
                          <para>
                              The gap parameter. Default is 4 GiB. The
                              size of free space below which it will
                              be assumed that the pool is full and
                              consequently the least recently used
                              file has to be removed. If, on the other
                              hand, the free space is greater than
                              <varname>gapPara</varname>, it will be
                              expensive to store a file on the pool
                              which exceeds the free space.
                          </para>
                          <para>
                              It can be set per pool with the <xref
                              linkend="cmd-set_gap"/> command. This
                              has to be done in the pool cell and
                              not in the pool manager
                              cell. Nevertheless it only influences
                              the cost calculation scheme within the
                              pool manager and not the bahaviour of
                              the pool itself.
                          </para>
                      </listitem>
                  </varlistentry>
                  <varlistentry>
                      <term><phrase role="math">costForMinute</phrase></term>
                      <listitem>
                          <para>
                              A parameter which fixes the space cost
                              of a one-minute-old LRU file to
                              <phrase role="math">(1 +
                              costForMinute)</phrase>.  It can be
                              set with the <xref
                              linkend="cmd-set_breakeven"/>, where
                          </para>

                          <para>
                              costForMinute = breakeven * 7 * 24 * 60.
                          </para>

                          <para>
                              I.e. the the space cost of a
                              one-week-old LRU file will be <phrase
                              role="math">(1 +
                              breakeven)</phrase>. Note again, that
                              all this only applies if <phrase
                              role="math">breakeven &lt;
                              1.0</phrase>
                          </para>
                      </listitem>
                  </varlistentry>
              </variablelist>

              The prescription above can be stated a little differently as follows:

              <informaltable frame="none">
                  <tgroup cols="4">
                      <colspec colnum="1" colname="if"       colwidth="20"/>
                      <colspec colnum="2" colname="formula"  colwidth="*"/>
                      <colspec colnum="3" colname="then"     colwidth="25"/>
                      <colspec colnum="4" colname="formula2" colwidth="2*"/>
                      <tbody>
                          <row>
                              <entry>If </entry>
                              <entry>
                                  freeSpace &gt; gapPara
                              </entry>
                              <entry> then </entry>
                              <entry>
                                  spaceCost = 3 * newFileSize / freeSpace
                              </entry>
                          </row>
                          <row>
                              <entry>If </entry>
                              <entry>
                                  freeSpace &lt;= gapPara
                              </entry>
                              <entry> then </entry>
                              <entry>
                                  spaceCost = 1 + breakeven * 7 * 24 * 60 * 60 / lruAge
                                  ,
                              </entry>
                          </row>
                      </tbody>
                  </tgroup>
              </informaltable>

              where <varname>newFileSize</varname> is at least 50MB and
              <varname>lruAge</varname> at least one minute.
          </para>

          <section>
              <title>Rationale</title>

              <!-- <para>
                   NOCLUE
                   </para> -->

              <para>
                  As the last version of the formula suggests, a
                  pool can be in two states: Either <phrase
                  role="math">freeSpace &gt; gapPara</phrase> or
                  <phrase role="math">freeSpace &lt;=
                  gapPara</phrase> - either there is free space left
                  to store files without deleting cached files or
                  there isn't.
              </para>

              <para>
                  Therefore, <varname>gapPara</varname> should be
                  around the size of the smallest files which
                  frequently might be written to the pool. If files
                  smaller than <varname>gapPara</varname> appear
                  very seldom or never, the pool might get stuck in
                  the first of the two cases with a high cost.
              </para>

              <para>
                  If the LRU file is smaller than the new file,
                  other files might have to be deleted. If these are
                  much younger than the LRU file, this space cost
                  calculation scheme might not lead to a selection
                  of the optimal pool. However, in pratice this
                  happens very seldomly and this scheme turns out to
                  be very efficient.
              </para>

          </section>

      </section>

      <section id="cf-pm-classic-total">
          <title>The Total Cost</title>

          <para>
              The total cost is a linear combination of the
              <glossterm
                  linkend="gl-performance_cost">performance</glossterm>
              and <glossterm linkend="gl-space_cost">space
              cost</glossterm>.

              I.e.

              <!--
                  <screen><varname>cost</varname> = <varname>ccf</varname> * <varname>performance_cost</varname> + <varname>scf</varname> * <varname>space_cost</varname>,</screen>
              -->
              <!--	<m:math xmlns:m="http://www.w3.org/1998/Math/MathML">
                  <m:mrow>
                  <m:mi>totalCost</m:mi>
                  <m:mo>=</m:mo>
                  <m:mi>ccf</m:mi>
                  <m:mo>*</m:mo>
                  <m:mi>perfCost</m:mi>
                  <m:mo>+</m:mo>
                  <m:mi>scf</m:mi>
                  <m:mo>*</m:mo>
                  <m:mi>spaceCost</m:mi>
                  </m:mrow>
                  </m:math>,-->
              <!--
                  <m:math xmlns:m="http://www.w3.org/1998/Math/MathML">
                  <m:mrow>
                  <m:apply>
                  <m:eq/>
                  <m:ci>totalCost</m:ci>
                  <m:apply>
                  <m:plus/>
                  <m:apply>
                  <m:times/>
                  <m:ci>ccf</m:ci>
                  <m:ci>perfCost</m:ci>
                  </m:apply>
                  <m:apply>
                  <m:times/>
                  <m:ci>scf</m:ci>
                  <m:ci>spaceCost</m:ci>
                  </m:apply>
                  </m:apply>
                  </m:apply>
                  </m:mrow>
                  </m:math>
              -->
              totalCost = ccf * perfCost + scf * spaceCost ,


              where <varname>ccf</varname> and <varname>scf</varname> are
              configurable with the command <xref
              linkend="cmd-set_pool_decision"/>.

              E.g.,

              <screen>&dc-prompt-pm; <userinput>set pool decision <option>-spacecostfactor=3</option> <option>-cpucostfactor=1</option></userinput></screen>

              will give the <glossterm linkend="gl-space_cost">space
              cost</glossterm> three times the weight of the <glossterm
              linkend="gl-performance_cost">performance cost</glossterm>.
          </para>
      </section>

      <section id="cf-pm-classic-parameters">
          <title>Parameters of Classic Partitions</title>

          <para>Classic partitions have a large number of tunable
          parameters. These parameters are set using the <command>pm
          set</command> command.</para>

          <informalexample>
              <para>To set the space cost factor on the
              <literal>default</literal> partition to
              <literal>0.3</literal>, use the following
              command:</para>
              <programlisting>
                  <command>pm set</command> default -spacecostfactor=0.3
              </programlisting>
          </informalexample>

          <informaltable>
              <tgroup cols="3" align="left">
                  <colspec colnum="1"  colwidth="13em"/>
                  <colspec colnum="2"  colwidth="*"/>
                  <colspec colnum="3"  colwidth="4em"/>
                  <thead>
                      <row>
                          <entry>Command</entry>
                          <entry>Meaning</entry>
                          <entry>Type</entry>
                      </row>
                  </thead>

                  <tbody>

                      <row>
                          <entry>
                              <cmdsynopsis>
                                  <command>pm set</command>
                                  <arg><replaceable>partitionName</replaceable></arg>
                                  <arg choice='plain'>-spacecostfactor=<replaceable>scf</replaceable></arg>
                              </cmdsynopsis>
                          </entry>
                          <entry>
                              <para>
                                  Sets the <literal>space cost factor</literal> for the partition.
                              </para>
                              <para>
                                  The default value is <literal>1.0</literal>.
                              </para>
                          </entry>
                          <entry>
                              float
                          </entry>
                      </row>

                      <row>
                          <entry>
                              <cmdsynopsis>
                                  <command>pm set</command>
                                  <arg><replaceable>partitionName</replaceable></arg>
                                  <arg choice='plain'>-cpucostfactor=<replaceable>ccf</replaceable></arg>
                              </cmdsynopsis>
                          </entry>
                          <entry>
                              <para>
                                  Sets the cpu cost factor for the partition.
                              </para>
                              <para>
                                  The default value is <literal>1.0</literal>.
                              </para>
                          </entry>
                          <entry>
                              float
                          </entry>
                      </row>

                      <row>
                          <entry>
                              <cmdsynopsis>
                                  <command>pm set</command>
                                  <arg><replaceable>partitionName</replaceable></arg>
                                  <arg choice='plain'>-idle=<replaceable>idle-value</replaceable></arg>
                              </cmdsynopsis>
                          </entry>
                          <entry>
                              <para>
                                  The concept of the <firstterm>idle value</firstterm>
                                  will be turned on if
                                  <replaceable>idle-value</replaceable> &gt;
                                  <literal>0.0</literal>.
                              </para>
                              <para>
                                  A pool is idle if its performance cost is smaller than
                                  the <replaceable>idle-value</replaceable>. Otherwise
                                  it is not idle.
                              </para>
                              <para>
                                  If one or more pools that satisfy the read request are
                                  idle then only one of them is chosen for a particular
                                  file irrespective of total cost. I.e. if the same file
                                  is requested more than once it will always be taken
                                  from the same pool. This allowes the copies on the
                                  other pools to age and be garbage collected.
                              </para>
                              <para>
                                  The default value is <literal>0.0</literal>, which
                                  disables this feature.
                              </para>
                          </entry>
                          <entry>
                              float
                          </entry>
                      </row>

                      <row>
                          <entry>
                              <cmdsynopsis>
                                  <command>pm set</command>
                                  <arg><replaceable>partitionName</replaceable></arg>
                                  <arg choice='plain'>-p2p=<replaceable>p2p-value</replaceable></arg>
                              </cmdsynopsis>
                          </entry>
                          <entry>
                              <para>
                                  Sets the static replication threshold for the
                                  partition.
                              </para>
                              <para>
                                  If the performance cost on the best pool exceeds
                                  <replaceable>p2p-value</replaceable> and the value for
                                  <link
                                      linkend='slope'><replaceable>slope</replaceable></link>
                                  = <literal>0.0</literal> then this pool is called
                                  <firstterm>hot</firstterm> and a pool to pool
                                  replication may be triggered.
                              </para>
                              <para>
                                  The default value is <literal>0.0</literal>, which
                                  disables this feature.
                              </para>
                          </entry>
                          <entry>
                              float
                          </entry>
                      </row>

                      <row>
                          <entry>
                              <cmdsynopsis>
                                  <command>pm set</command>
                                  <arg><replaceable>partitionName</replaceable></arg>
                                  <arg choice='plain'>-alert=<replaceable>value</replaceable></arg>
                              </cmdsynopsis>
                          </entry>
                          <entry>
                              <para>
                                  Sets the <firstterm>alert value</firstterm> for the
                                  partition.
                              </para>
                              <para>
                                  If the best pool's performance cost exceeds the p2p
                                  value and the alert value then no pool to pool copy is
                                  triggered and a message will be logged stating that no
                                  pool to pool copy will be made.
                              </para>
                              <para>
                                  The default value is <literal>0.0</literal>, which
                                  disables this feature.
                              </para>
                          </entry>
                          <entry>
                              float
                          </entry>
                      </row>

                      <row>
                          <entry>
                              <cmdsynopsis>
                                  <command>pm set</command>
                                  <arg><replaceable>partitionName</replaceable></arg>
                                  <arg choice='plain'>-panic=<replaceable>value</replaceable></arg>
                              </cmdsynopsis>
                          </entry>
                          <entry>
                              <para>
                                  Sets the <firstterm>panic cost cut level</firstterm>
                                  for the partition.
                              </para>
                              <para>
                                  If the performance cost of the best pool exceeds the
                                  panic cost cut level the request will fail.
                              </para>
                              <para>
                                  The default value is <literal>0.0</literal>, which
                                  disables this feature.
                              </para>
                          </entry>
                          <entry>
                              float
                          </entry>
                      </row>

                      <row>
                          <entry>
                              <cmdsynopsis>
                                  <command>pm set</command>
                                  <arg><replaceable>partitionName</replaceable></arg>
                                  <arg choice='plain'>-fallback=<replaceable>value</replaceable></arg>
                              </cmdsynopsis>
                          </entry>
                          <entry>
                              <para>
                                  Sets the fallback cost cut level for the partition.
                              </para>
                              <para>
                                  If the best pool's performance cost exceeds the
                                  fallback cost cut level then a pool of the next
                                  <firstterm>level</firstterm> will be chosen. This
                                  means for example that instead of choosing a pool with
                                  <literal>readpref</literal> = 20 a pool with
                                  <literal>readpref</literal> &lt; 20 will be chosen.
                              </para>
                              <para>
                                  The default value is <literal>0.0</literal>, which disables this feature.
                              </para>
                          </entry>
                          <entry>
                              float
                          </entry>
                      </row>

                      <row>
                          <entry id='slope'>
                              <cmdsynopsis>
                                  <command>pm set</command>
                                  <arg><replaceable>partitionName</replaceable></arg>
                                  <arg choice='plain'>-slope=<replaceable>slope</replaceable></arg>
                              </cmdsynopsis>
                          </entry>
                          <entry>
                              <para>
                                  Sets the dynamic replication threshold value for the
                                  partition.
                              </para>
                              <para>
                                  If <replaceable>slope</replaceable>&gt;
                                  <literal>0.01</literal> then the product of best
                                  pool's performance cost and
                                  <replaceable>slope</replaceable> is used as threshold
                                  for pool to pool replication.
                              </para>
                              <para>
                                  If the performance cost on the best pool exceeds this
                                  threshold then this pool is called hot.
                              </para>

                              <para>
                                  The default value is <literal>0.0</literal>, which disables this feature.
                              </para>
                          </entry>
                          <entry>
                              float
                          </entry>
                      </row>

                      <row>
                          <entry>
                              <cmdsynopsis>
                                  <command>pm set</command>
                                  <arg><replaceable>partitionName</replaceable></arg>
                                  <arg choice='plain'>-p2p-allowed=<replaceable>value</replaceable></arg>
                              </cmdsynopsis>
                          </entry>
                          <entry>
                              <para>
                                  This value can be specified if an &hsm; is attached to
                                  the &dcache;.
                              </para>
                              <para>
                                  If a partition has no &hsm; connected, then this
                                  option is overridden. This means that no matter which
                                  value is set for <literal>p2p-allowed</literal> the
                                  pool to pool replication will always be allowed.
                              </para>
                              <para>
                                  By setting <replaceable>value</replaceable> =
                                  <literal>off</literal> the values for
                                  <literal>p2p-allowed</literal>,
                                  <literal>p2p-oncost</literal> and
                                  <literal>p2p-fortransfer</literal> will take over the
                                  value of the default partition.
                              </para>
                              <para>
                                  If <replaceable>value</replaceable> =
                                  <literal>yes</literal> then pool to pool replication
                                  is allowed.
                              </para>
                              <para>
                                  As a side effect of setting <replaceable>value</replaceable> =
                                  <literal>no</literal> the values for
                                  <literal>p2p-oncost</literal> and
                                  <literal>p2p-fortransfer</literal> will also be set to
                                  <literal>no</literal>.
                              </para>
                              <para>
                                  The default value is <literal>yes</literal>.
                              </para>
                          </entry>
                          <entry>
                              boolean
                          </entry>
                      </row>

                      <row>
                          <entry>
                              <cmdsynopsis>
                                  <command>pm set</command>
                                  <arg><replaceable>partitionName</replaceable></arg>
                                  <arg choice='plain'>-p2p-oncost=<replaceable>value</replaceable></arg>
                              </cmdsynopsis>
                          </entry>
                          <entry>
                              <para>
                                  Determines whether pool to pool replication is allowed
                                  if the best pool for a read request is hot.
                              </para>
                              <para>
                                  The default value is <literal>no</literal>.
                              </para>
                          </entry>
                          <entry>
                              boolean
                          </entry>
                      </row>

                      <row>
                          <entry>
                              <cmdsynopsis>
                                  <command>pm set</command>
                                  <arg><replaceable>partitionName</replaceable></arg>
                                  <arg choice='plain'>-p2p-fortransfer=<replaceable>value</replaceable></arg>
                              </cmdsynopsis>
                          </entry>
                          <entry>
                              <para>
                                  If the best pool is hot and the requested file will be
                                  copied either from the hot pool or from tape to
                                  another pool, then the requested file will be read
                                  from the pool where it just had been copied to if
                                  <replaceable>value</replaceable> = <literal>yes</literal>.
                                  If
                                  <replaceable>value</replaceable> = <literal>no</literal>
                                  then the requested file will be read from the hot
                                  pool.
                              </para>
                              <para>
                                  The default value is <literal>no</literal>.
                              </para>
                          </entry>
                          <entry>
                              boolean
                          </entry>
                      </row>

                      <row>
                          <entry>
                              <cmdsynopsis>
                                  <command>pm set</command>
                                  <arg><replaceable>partitionName</replaceable></arg>
                                  <arg choice='plain'>-stage-allowed=<replaceable>value</replaceable></arg>
                              </cmdsynopsis>
                          </entry>
                          <entry>
                              <para>
                                  Set the <firstterm>stage allowed value</firstterm> to
                                  <literal>yes</literal> if a tape system is connected
                                  and to <literal>no</literal> otherwise.
                              </para>
                              <para>
                                  As a side effect, setting the value for
                                  <literal>stage-allowed</literal> to
                                  <literal>no</literal> changes the value for
                                  <literal>stage-oncost</literal> to
                                  <literal>no</literal>.
                              </para>
                              <para>
                                  The default value is <literal>no</literal>.
                              </para>
                          </entry>
                          <entry>
                              boolean
                          </entry>
                      </row>

                      <row>
                          <entry>
                              <cmdsynopsis>
                                  <command>pm set</command>
                                  <arg><replaceable>partitionName</replaceable></arg>
                                  <arg choice='plain'>-stage-oncost=<replaceable>value</replaceable></arg>
                              </cmdsynopsis>
                          </entry>
                          <entry>
                              <para>
                                  If the best pool is hot, p2p-oncost is disabled and an
                                  &hsm; is connected to a pool then this parameter
                                  determines whether to stage the requested file to a
                                  different pool.
                              </para>
                              <para>
                                  The default value is <literal>no</literal>.
                              </para>
                          </entry>
                          <entry>
                              boolean
                          </entry>
                      </row>

                      <row>
                          <entry>
                              <cmdsynopsis>
                                  <command>pm set</command>
                                  <arg><replaceable>partitionName</replaceable></arg>
                                  <arg choice='plain'>-max-copies=<replaceable>copies</replaceable></arg>
                              </cmdsynopsis>
                          </entry>
                          <entry>
                              <para>
                                  Sets the maximal number of replicas of one file. If the maximum is
                                  reached no more replicas will be created.
                              </para>
                              <para>
                                  The default value is <literal>500</literal>.
                              </para>
                          </entry>
                          <entry>
                              integer
                          </entry>
                      </row>

                  </tbody>
              </tgroup>
          </informaltable>

	</section>

      </section>

  </section>



    <section id='cf-pm-linkgroups'>
      <title>Link Groups</title>

	<para>
	  The &cell-poolmngr; supports a type of objects called
	  <firstterm>link groups</firstterm>. These link groups are
	  used by the <link linkend='cf-srm-space'>&srm;
	  &cell-spacemngr;</link> to make reservations against
	  space. Each link group corresponds to a number of &dcache;
	  pools in the following way: A link group is a collection of
	  <link linkend='cf-pm-links'>links</link> and each link
	  points to a set of pools. Each link group knows about the
	  size of its available space, which is the sum of all sizes
	  of available space in all the pools included in this link
	  group.
         </para>

	   <para>
	     To create a new link group login to the <link
	     linkend='intouch-admin'>Admin Interface</link> and
	     <command>cd</command> to the &cell-poolmngr;.
	   </para>
	   <screen>&dc-prompt-local; <userinput>cd PoolManager</userinput>
&dc-prompt-pm; <userinput>psu create linkGroup <replaceable>linkgroup</replaceable></userinput>
&dc-prompt-pm; <userinput>psu addto linkGroup <replaceable>linkgroup</replaceable> <replaceable>link</replaceable></userinput>
&dc-prompt-pm; <userinput>save</userinput></screen>

           <para>
	     With <command>save</command> the changes will be saved to
	     the file
	     <filename>&file-poolmanager;</filename>.
	   </para>

	   <note>
	     <para>
	       You can also edit the file
	       <filename>&file-poolmanager;</filename>
	       to create a new link group. Please make sure that it
	       already exists. Otherwise you will have to create it
	       first via the Admin Interface by
	     </para>
	     <screen>&dc-prompt-pm; <userinput>save</userinput></screen>

	     <para>
	       Edit the file
	       <filename>&file-poolmanager;</filename>
	     </para>

	     <screen><command>psu create linkGroup</command> <replaceable>linkgroup</replaceable>
<command>psu addto linkGroup</command> <replaceable>linkgroup</replaceable> <replaceable>link</replaceable></screen>

             <para>
	       After editing this file you will have to restart the
	       domain which contains the &cell-poolmngr; cell to apply
	       the changes.
	     </para>
	   </note>

         <note>
	   <para>
	     Administrators will have to take care, that no pool is
	     present in more than one link group.
	   </para>
	 </note>

      <formalpara>
          <title>Access latency and retention policy</title>
          <para>
              A space reservation has a
              <firstterm>retention policy</firstterm> and
              an <firstterm>access latency</firstterm>, where
              retention policy describes the quality of
              the storage service that will be provided for files in
              the space reservation and
              access latency describes the availability of
              the files. See <xref linkend='cf-srm-intro-spaceReservation'/>
              for further details.</para>
      </formalpara>
      <para>
          A link group has five boolean properties called
          <varname>replicaAllowed</varname>,
          <varname>outputAllowed</varname>,
          <varname>custodialAllowed</varname>,
          <varname>onlineAllowed</varname> and
          <varname>nearlineAllowed</varname>, which determine the access latencies and
          retention policies allowed in the link group. The values of these
          properties (<literal>true</literal> or
          <literal>false</literal>) can be configured via the Admin
          Interface or directly in the file
	<filename>&file-poolmanager;</filename>.
      </para>
        <para>
            For a space reservation to be allowed in a link group, the
            the retention policy and access latency
            of the space reservation must be allowed in the link group.
        </para>

      <screen>&dc-prompt-pm; <userinput>psu set linkGroup custodialAllowed <replaceable>linkgroup</replaceable> <replaceable>true|false</replaceable></userinput>
&dc-prompt-pm; <userinput>psu set linkGroup outputAllowed <replaceable>linkgroup</replaceable> <replaceable>true|false</replaceable></userinput>
&dc-prompt-pm; <userinput>psu set linkGroup replicaAllowed <replaceable>linkgroup</replaceable> <replaceable>true|false</replaceable></userinput>
&dc-prompt-pm; <userinput>psu set linkGroup onlineAllowed <replaceable>linkgroup</replaceable> <replaceable>true|false</replaceable></userinput>
&dc-prompt-pm; <userinput>psu set linkGroup nearlineAllowed <replaceable>linkgroup</replaceable> <replaceable>true|false</replaceable></userinput></screen>

        <important>
            <para>
                It is up to the administrator to ensure that the
                link groups' properties are specified correctly.
            </para>
            <para>
                For example &dcache; will not complain if a link group
                that does not support a tape backend will be declared as one
                that supports <literal>custodial</literal> files.
            </para>
            <para>
                It is essential that space in a link group is homogeneous with respect
                to access latencies, retention policies and storage groups accepted.
                Otherwise space reservations cannot be guaranteed. For instance, if
                only a subset of pools accessible through a link group support custodial
                files, there is no guarantee that a custodial space reservation created
                within the link group will fit on those pools.
            </para>
        </important>
    </section>


</chapter>
